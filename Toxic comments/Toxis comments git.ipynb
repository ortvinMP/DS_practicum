{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66fc0c5f",
   "metadata": {},
   "source": [
    "# Проект - Викишоп"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2d985b",
   "metadata": {},
   "source": [
    "## Описание проекта\n",
    "\n",
    "Интернет-магазин запускает новый сервис. Пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "Обучим модель классифицировать комментарии на позитивные и негативные. В нашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "Необходимо построить модель со значением метрики качества F1 не меньше 0.75. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296dbc36",
   "metadata": {},
   "source": [
    "# Выполнение проекта"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c747c4ed",
   "metadata": {},
   "source": [
    "## Выгрузка необходимых библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f8a4c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\kaz-106\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kaz-106\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\kaz-106\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kaz-106\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kaz-106\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score,classification_report\n",
    "from tqdm import tqdm\n",
    "import lightgbm as lgb\n",
    "import transformers\n",
    "from tqdm import notebook\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBRegressor\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476a851d",
   "metadata": {},
   "source": [
    "## Загрузка данных\n",
    "\n",
    "- Загрузим данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e24a2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    data=pd.read_csv('C:/Users/kaz-106/YandexDisk/Py-projects/02_Practicum projects/11_Machine_learning_for_texts/toxic_comments.csv')\n",
    "except:\n",
    "    data=pd.read_csv('/datasets/toxic_comments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98cdb53",
   "metadata": {},
   "source": [
    "## Предобработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf8dacc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0     int64\n",
      "text          object\n",
      "toxic          int64\n",
      "dtype: object\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159292 entries, 0 to 159291\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   Unnamed: 0  159292 non-null  int64 \n",
      " 1   text        159292 non-null  object\n",
      " 2   toxic       159292 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 3.6+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic\n",
       "0           0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1           1  D'aww! He matches this background colour I'm s...      0\n",
       "2           2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3           3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4           4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(data.dtypes)\n",
    "print(data.info())\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ae9574",
   "metadata": {},
   "source": [
    "### Лемматизация, очистка от лишних символов и пробелов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2065eb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    tokenized = nltk.word_tokenize(text)\n",
    "    joined = ' '.join(tokenized)\n",
    "    texted = re.sub(r'[^a-zA-Z]', ' ', joined)\n",
    "    resulted = ' '.join(texted.split())\n",
    "    return resulted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e5a043c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lemm_text'] = data['text'].apply(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7311600e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def nltk_pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_pos_tagger(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    \n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11e3a8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 159292/159292 [10:18<00:00, 257.49it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "data['lemm_text'] = data['text'].progress_apply(lemmatize_sentence) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13573703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>lemm_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>Explanation Why the edits make under my userna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>D'aww ! He match this background colour I 'm s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>Hey man , I 'm really not try to edit war . It...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>`` More I ca n't make any real suggestion on i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>You , sir , be my hero . Any chance you rememb...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic  \\\n",
       "0           0  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1           1  D'aww! He matches this background colour I'm s...      0   \n",
       "2           2  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3           3  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4           4  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "                                           lemm_text  \n",
       "0  Explanation Why the edits make under my userna...  \n",
       "1  D'aww ! He match this background colour I 'm s...  \n",
       "2  Hey man , I 'm really not try to edit war . It...  \n",
       "3  `` More I ca n't make any real suggestion on i...  \n",
       "4  You , sir , be my hero . Any chance you rememb...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4795930",
   "metadata": {},
   "source": [
    "## Подготовка и обучение моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a71ee5b",
   "metadata": {},
   "source": [
    "### Разделение датасетов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcadb9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data['lemm_text']\n",
    "target = data['toxic']\n",
    "features_train,features_test,target_train,target_test=train_test_split(features,target, test_size=0.2,random_state=12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6501d0",
   "metadata": {},
   "source": [
    "### Создание корпусов слов и установка стоп-слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "383415f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"Bushranger you 're a GRASS with no sense of humour . Seen the South Park episode Poor and Stupid ? That 's what I be refer to , a comment obviously lose on you . But you 've show you ca n't fight your own battle and have to run cry to mummy - boo-hoo.90.204.13.4\",\n",
       "       \"`` Need administrative help I have be block , iniquitously . Read the above request . I try to explain everything there . Hegemonic use of administrative privilege ought to not be overlook . Ask Administrator if I be harass him to such an extent where I need to be block from edit , that too for 48 hr . For goodness ' sake . What happen to do n't bite the newcomer ? Please talk to Bwilkins . See the talk page of he have a knack of be unreasonable with aplomb . here ``\",\n",
       "       \"I 'd also like to point out that he have use a third-person plural pronoun and possessive pronoun to refer to one person ( me ) and one person 's ( my ) possession .\",\n",
       "       ...,\n",
       "       \"`` Agreed . We really should try to stick to the subject of the article at hand . And look through the Jeremy Clarkson article I see there really be n't a lot about his penchant for speed . It 's mention , but just briefly , alongside his clean driving record . However in search via Google , I find this article , http : //driving.timesonline.co.uk/article/0 , ,12529-1890873_2,00.html This to me seem rather striking , to have somebody talk so openly about speed . Maybe we could change the article around to be , In spite of his penchant for speed , Clarkson hold a clean driving licence . None the less , he be not especially reluctant to discuss the subject of speed . In a November 2005 article in `` '' The Sunday Times '' '' , Mr. Clarkson write , while discuss the Bugatti Veyron , '' '' On a recent drive across Europe I desperately want to reach the top speed but I run out of road when the needle hit 240mph '' '' , and later , in the same article , `` '' From behind the wheel of a Veyron , France be the size of a small coconut . I can not tell you how fast I cross it the other day . Because you simply wouldn ’ t believe me '' '' . Could we write that ? These two quote be rather remarkable , in my opinion , and worth mentioning in the article . They demonstrate his honest and frank manner of discuss his travel across Europe . And we could just leave it to the reader to decide if he be talk about go 240mph on public road , or whatever . I realize you may feel differently about Mr. Clarkson than I do , but none the less , these two quote do clarify his penchant for speed , and be worth mentioning , alongside his clean driving record , I would say. ``\",\n",
       "       '`` Umm killer Do you not like that he copy your whole userpage ? I put it up for deletion but it be keep with one vote . Cheers , tLover ( ) ``',\n",
       "       'Bradford City I be remove unreferanced content .'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "features_train_corpus = features_train.values\n",
    "features_test_corpus = features_test.values\n",
    "\n",
    "display(features_train_corpus)\n",
    "\n",
    "stopwords = set(nltk_stopwords.words('english'))\n",
    "display(len(stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114d7da1",
   "metadata": {},
   "source": [
    "## Препроцессинг - векторизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2ebbe0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127433, 158268)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models=[]\n",
    "results=[]\n",
    "\n",
    "count_vect = CountVectorizer(stop_words=stopwords, dtype=np.float32)\n",
    "\n",
    "bow_features_train = count_vect.fit_transform(features_train_corpus)\n",
    "bow_features_test = count_vect.transform(features_test_corpus)\n",
    "\n",
    "display(bow_features_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e98a3f5",
   "metadata": {},
   "source": [
    "### Логистическая регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab42989e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_Logistic = LogisticRegression(max_iter=1000)\n",
    "#model_Logistic.fit(bow_features_train,target_train)\n",
    "\n",
    "#test_f1_score = f1_score(target_test, model_Logistic.predict(bow_features_test))\n",
    "\n",
    "#print(\"Test F1 score (LogisticRegression):\",test_f1_score)   \n",
    "#models.append('Vect_LogisticRegression')\n",
    "#results.append(test_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25f714c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logreg(features_train, target_train, features_test, target_test):\n",
    "\n",
    "    logreg = LogisticRegression()\n",
    "    \n",
    "    parameters = {'C': [0.1,1],\n",
    "                 'solver': ['lbfgs','liblinear'],\n",
    "                  'penalty': ['l2']}\n",
    "    \n",
    "    grid_search_logreg = GridSearchCV(logreg, parameters, cv=5, n_jobs=-1)\n",
    "    \n",
    "    grid_search_logreg.fit(features_train, target_train)\n",
    "    \n",
    "    print(grid_search_logreg.best_params_)\n",
    "    print(classification_report(target_test, grid_search_logreg.predict(features_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1918ce01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98     28629\n",
      "           1       0.86      0.68      0.76      3230\n",
      "\n",
      "    accuracy                           0.96     31859\n",
      "   macro avg       0.91      0.83      0.87     31859\n",
      "weighted avg       0.95      0.96      0.95     31859\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logreg(bow_features_train,target_train, bow_features_test, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a326ed7e",
   "metadata": {},
   "source": [
    "### LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24890b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_LGBM =lgb.LGBMClassifier(n_estimators = 1000, learning_rate = 0.05)\n",
    "#model_LGBM.fit(bow_features_train,target_train)\n",
    "\n",
    "#test_f1_score = f1_score(target_test, model_LGBM.predict(bow_features_test))\n",
    "\n",
    "#print(\"Test F1 score (LGBM):\",test_f1_score)\n",
    "#models.append('Vect_LGBMClassifier')\n",
    "#results.append(test_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "704e48c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbm(features_train, target_train, features_test, target_test):\n",
    "    \n",
    "    lgbm_model = lgb.LGBMClassifier()\n",
    "    \n",
    "    parameters = {'max_depth': [5,10],\n",
    "                 'n_estimators': [5,10],\n",
    "                  'learning_rate': [5,10]}\n",
    "    \n",
    "    grid_search_lgbm = GridSearchCV(lgbm_model, parameters, cv=5, n_jobs=-1)\n",
    "    \n",
    "    grid_search_lgbm.fit(features_train, target_train)\n",
    "    \n",
    "    print(grid_search_lgbm.best_params_)\n",
    "    print(classification_report(target_test, grid_search_lgbm.predict(features_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3a80ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 5, 'max_depth': 10, 'n_estimators': 5}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.00      0.00     28629\n",
      "           1       0.10      1.00      0.18      3230\n",
      "\n",
      "    accuracy                           0.10     31859\n",
      "   macro avg       0.35      0.50      0.09     31859\n",
      "weighted avg       0.55      0.10      0.02     31859\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lgbm(bow_features_train,target_train, bow_features_test, target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0dee05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr = lgb.LGBMClassifier()\n",
    "#print(lr.get_params().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352242a5",
   "metadata": {},
   "source": [
    "### CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c354a4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CatBoost_model = CatBoostClassifier(learning_rate=0.1, max_depth=10,iterations=10)\n",
    "#CatBoost_model.fit(bow_features_train, target_train)\n",
    "\n",
    "#test_f1_score = f1_score(target_test, CatBoost_model.predict(bow_features_test))\n",
    "\n",
    "#print(\"Test F1 score (LGBM):\",test_f1_score)\n",
    "#models.append('Vect_CatBoostClassifier')\n",
    "#results.append(test_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f550b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def catboost(features_train, target_train, features_test, target_test):\n",
    "    \n",
    "    cat_model = CatBoostClassifier()\n",
    "    \n",
    "    parameters = {'depth': [2,4],\n",
    "                 'iterations': [5],\n",
    "                  'learning_rate': [0.1,0.5]}\n",
    "    \n",
    "    grid_search_cat= GridSearchCV(cat_model, parameters, cv=5, n_jobs=-1)\n",
    "    \n",
    "    grid_search_cat.fit(features_train, target_train)\n",
    "    \n",
    "    print(grid_search_cat.best_params_)\n",
    "    print(classification_report(target_test, grid_search_cat.predict(features_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d1060b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.3431463\ttotal: 670ms\tremaining: 2.68s\n",
      "1:\tlearn: 0.2699141\ttotal: 1.23s\tremaining: 1.84s\n",
      "2:\tlearn: 0.2444803\ttotal: 1.78s\tremaining: 1.19s\n",
      "3:\tlearn: 0.2312446\ttotal: 2.34s\tremaining: 585ms\n",
      "4:\tlearn: 0.2247160\ttotal: 2.85s\tremaining: 0us\n",
      "{'depth': 4, 'iterations': 5, 'learning_rate': 0.5}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96     28629\n",
      "           1       0.93      0.32      0.48      3230\n",
      "\n",
      "    accuracy                           0.93     31859\n",
      "   macro avg       0.93      0.66      0.72     31859\n",
      "weighted avg       0.93      0.93      0.91     31859\n",
      "\n"
     ]
    }
   ],
   "source": [
    "catboost(bow_features_train,target_train, bow_features_test, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ad612e",
   "metadata": {},
   "source": [
    "## Препроцессинг - TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "384bfd96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127433, 158268)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(stop_words=stopwords)\n",
    "\n",
    "tfidf_features_train = tfidf.fit_transform(features_train_corpus)\n",
    "tfidf_features_test = tfidf.transform(features_test_corpus)\n",
    "\n",
    "display(tfidf_features_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c903ff6",
   "metadata": {},
   "source": [
    "### Логистическая регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80a3edde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98     28629\n",
      "           1       0.93      0.61      0.74      3230\n",
      "\n",
      "    accuracy                           0.96     31859\n",
      "   macro avg       0.94      0.80      0.86     31859\n",
      "weighted avg       0.95      0.96      0.95     31859\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logreg(tfidf_features_train,target_train, tfidf_features_test, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d765f2",
   "metadata": {},
   "source": [
    "### LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3a39e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 5, 'max_depth': 5, 'n_estimators': 5}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      1.00      0.95     28629\n",
      "           1       0.80      0.13      0.23      3230\n",
      "\n",
      "    accuracy                           0.91     31859\n",
      "   macro avg       0.85      0.56      0.59     31859\n",
      "weighted avg       0.90      0.91      0.88     31859\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lgbm(tfidf_features_train,target_train, tfidf_features_test, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3db589f",
   "metadata": {},
   "source": [
    "### CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e15b1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.3441069\ttotal: 670ms\tremaining: 2.68s\n",
      "1:\tlearn: 0.2678566\ttotal: 1.36s\tremaining: 2.04s\n",
      "2:\tlearn: 0.2447696\ttotal: 2.01s\tremaining: 1.34s\n",
      "3:\tlearn: 0.2332886\ttotal: 2.65s\tremaining: 664ms\n",
      "4:\tlearn: 0.2238455\ttotal: 3.35s\tremaining: 0us\n",
      "{'depth': 4, 'iterations': 5, 'learning_rate': 0.5}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96     28629\n",
      "           1       0.95      0.35      0.52      3230\n",
      "\n",
      "    accuracy                           0.93     31859\n",
      "   macro avg       0.94      0.68      0.74     31859\n",
      "weighted avg       0.93      0.93      0.92     31859\n",
      "\n"
     ]
    }
   ],
   "source": [
    "catboost(tfidf_features_train,target_train, tfidf_features_test, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb08991a",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b6581cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 score (LogisticRegression): 0.7570836212854181\n"
     ]
    }
   ],
   "source": [
    "model_Logistic = LogisticRegression(C=1.0, penalty=\"l2\", solver= 'lbfgs')\n",
    "model_Logistic.fit(bow_features_train,target_train)\n",
    "\n",
    "test_f1_score = f1_score(target_test, model_Logistic.predict(bow_features_test))\n",
    "\n",
    "print(\"Test F1 score (LogisticRegression):\",test_f1_score)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9b1c74",
   "metadata": {},
   "source": [
    "- Выполнена загрузка представленных текстовых данных и необходимых библиотек, загружены стоп-слова для соотвесвтующего языка (Английского)\n",
    "- Проведена предобработка и анализ данных, текст лемматизирован, очищен от лишних символов и пробелов\n",
    "- С помощью двух различных методов (векторизация и TF_IDF), подготовлены признаки для машинного обучения\n",
    "- Обучены три различные модели (LogisticRegression, LGBM и CatBoost) для различных методов препроцессинга\n",
    "- Наилучшее значение F1 показала модель LogisticRegression на CountVectoriser, это соотвествует критериям, указанных в условии задачи\n",
    "- BERT не был использован из-за сложности модели и ограниченности вычислительной мощности"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
